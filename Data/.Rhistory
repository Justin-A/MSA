SimulatedAnnealing(1000) # A : 4428, B : 5572
SimulatedAnnealing(10) # A : 286, B : 9714
286
SimulatedAnnealing(10) # A : 286, B : 9714
SimulatedAnnealing(10) # A : 288, B : 9712
SimulatedAnnealing(100) # A : 1879, B : 8121
SimulatedAnnealing(100) # A : 1813, B : 8187
SimulatedAnnealing(1000) # A : 4428, B : 5572
SimulatedAnnealing(10000) # A : 4966, B : 5034
SimulatedAnnealing(100000) # A : 5016, B : 4984
SimulatedAnnealing(1000000) # A : 4970, B : 5030
# Simulated Anneling
set.seed(1130)
SimulatedAnnealing <- function(x){
actions = c('A', 'B', 'C')
counts = c(A = 0, B = 0, C = 0)
mean = c(A = 1, B = 2, C = 3)
sd = c(A = 0.5, B = 1, C = 1.5)
pref = c(A = 0, B = 0, C = 0)
mean_rewards = 0
learning_rate = 0.1
T = x
for(t in 1:10000){
e = exp(pref/T) # T값을 추가하여 모의담금질 시도
p = e / sum(e)
chosen = sample(actions, 1, prob=p)
counts[chosen] = counts[chosen] + 1
print(chosen)
R = rnorm(1, mean[chosen], sd[chosen])
advantage = R - mean_rewards
pref[chosen] = pref[chosen] + learning_rate * advantage * (1 - p[chosen])
nc = setdiff(actions, chosen)
pref[nc] = pref[nc] - learning_rate * advantage * p[nc]
}
print(counts)
print(pref)
}
SimulatedAnnealing(10) # A : 288, B : 9712
SimulatedAnnealing(100) # A : 1798, B : 8202
SimulatedAnnealing(100) # A : 1853, B : 8147
SimulatedAnnealing(1000) # A : 4343, B : 5657
SimulatedAnnealing(10000) # A : 4878, B : 5122
SimulatedAnnealing(100000) # A : 5056, B : 4944
SimulatedAnnealing(1000000) # A : 5091, B : 4909
SimulatedAnnealing <- function(x){
actions = c('A', 'B', 'C', 'D', 'E')
counts = c(A = 0, B = 0, C = 0, D = 0, E = 0)
mean = c(A = 1, B = 2, C = 3, D = 4, E = 5)
sd = c(A = 0.5, B = 1, C = 1.5, D = 2, E = 2.5)
pref = c(A = 0, B = 0, C = 0, D = 0, E = 0)
mean_rewards = 0
learning_rate = 0.1
T = x
for(t in 1:10000){
e = exp(pref/T) # T값을 추가하여 모의담금질 시도
p = e / sum(e)
chosen = sample(actions, 1, prob=p)
counts[chosen] = counts[chosen] + 1
print(chosen)
R = rnorm(1, mean[chosen], sd[chosen])
advantage = R - mean_rewards
pref[chosen] = pref[chosen] + learning_rate * advantage * (1 - p[chosen])
nc = setdiff(actions, chosen)
pref[nc] = pref[nc] - learning_rate * advantage * p[nc]
}
print(counts)
print(pref)
}
SimulatedAnnealing(10) # A : 288, B : 9712
SimulatedAnnealing(100) # A : 1798, B : 8202
SimulatedAnnealing(100) # A : 1853, B : 8147
SimulatedAnnealing(1000) # A : 4343, B : 5657
SimulatedAnnealing(10) # A : 288, B : 9712
SimulatedAnnealing(100) # A : 1798, B : 8202
SimulatedAnnealing(100) # A : 1853, B : 8147
SimulatedAnnealing(1000) # A : 4343, B : 5657
SimulatedAnnealing(10000) # A : 4878, B : 5122
SimulatedAnnealing(100000) # A : 5056, B : 4944
SimulatedAnnealing(1000000) # A : 5091, B : 4909
SimulatedAnnealing(10) # A : 288, B : 9712
SimulatedAnnealing(100) # A : 1798, B : 8202, C :
SimulatedAnnealing(100) # A :
SimulatedAnnealing(1000) # A : 4343, B : 5657
SimulatedAnnealing(10000) # A :
SimulatedAnnealing(100000) # A : 5056, B : 4944
SimulatedAnnealing(1000000) # A : 5091, B : 4909
SimulatedAnnealing(10)      # A : 89,   B : 107,  C : 168,  D : 390,  E : 9246
10
set.seed(1130)
library(caret)
setwd("~/Desktop/Justin/2018-1/DataMining/Project/")
data.train <- read.csv("NA_Sample1.csv")
data.test <- read.csv("NA_Sample_Test1.csv")
control <- trainControl(method = 'cv', search = 'grid', number = 5)
xgb.grid = expand.grid(
.nrounds = 1000,
.max_depth = c(1:10),
.eta = c(seq(0.1,1,0.1)),
.gamma = 1,
.colsample_bytree = 1,
.min_child_weight = 1,
.subsample = 1
)
xgb.model <- train(
price ~ .,
data = data.train,
#tuneGrid = xgb.grid,
tuneLength = 10,
trControl = control,
method = 'xgbTree'
)
xgb.model <- train(
price ~ .,
data = data.train,
tuneLength = 10,
trControl = control,
method = 'xgbTree'
)
head(data.train)
dim(data.train)
xgb.model <- train(
price ~ .,
data = data.train,
tuneGrid = xgb.grid,
tuneLength = 10,
trControl = control,
method = 'xgbTree'
)
xgb.model <- train(
price ~ .,
data = data.train,
tuneLength = 10,
trControl = control,
method = 'xgbTree'
)
rm(list=ls())
set.seed(1130)
library(caret)
setwd("~/Desktop/Justin/2018-1/DataMining/Project/")
data.train <- read.csv("NA_Sample1.csv")
data.test <- read.csv("NA_Sample_Test1.csv")
control <- trainControl(method = 'cv', search = 'grid', number = 5)
xgb.model <- train(
price ~ .,
data = data.train,
tuneGrid = xgb.grid,
tuneLength = 1,
trControl = control,
method = 'xgbTree'
)
xgb.model <- train(
price ~ .,
data = data.train,
#tuneGrid = xgb.grid,
tuneLength = 1,
trControl = control,
method = 'xgbTree'
)
xgb.model <- train(
price ~ .,
data = data.train,
tuneLength = 1,
trControl = control,
method = 'xgbTree'
)
xgb.model
price.xgb = predict(xgb.model, data.test)
xgb.model
price.xgb = predict(xgb.model, data.test)
xgb.model <- train(
price ~ .,
data = data.train,
tuneLength = 10,
trControl = control,
method = 'xgbTree'
)
xgb.model <- train(
price ~ .,
data = data.train,
tuneLength = 10,
trControl = control,
method = 'xgbTree'
)
xgb.model
price.xgb = predict(xgb.model, data.test)
RMSE(data.test$price, price.xgb)
price.xgb = predict(xgb.model, data.test)
data.test <- read.csv("NA_Sample_Test1.csv")
price.xgb = predict(xgb.model, data.test)
save(xgb.model, "xgb.model2.R")
xgb.model
save(xgb.model, "xgb.model_2.R")
save(xgb.model, "xgb.model2.R")
xgb.model
xgb.model
save(xgb.model, "k.R")
?save
save(xgb.model, 'xgb.model_2.r')
dir()
model.extract()
model
model <- xgb.model
save(model, "xgb.model_2.R")
load("xgb.model.R")
xgb.model
price.xgb <- predict(xgb.model, data.test)
model
save(model, "model.R")
save(model)
save(model, file = "model.R")
rm(list=ls())
load("model_xgb1")
load("model_xgb1.R")
data.train <- read.csv("NA_Sample1.csv")
data.test <- read.csv("NA_Sample_Test1.csv")
head(data.trai )
head(data.train)
xgb.model
predict(xgb.model, data.test)
head(data.train)
View(data.train)
View(data.test)
data.train <- data.train[,-1]
colnames(data.train)
colnames(data.test)
xgb.model
rm(list=ls())
load(model_xgb2.R)
load("model_xgb2.R")
data.train <- read.csv("NA_Sample1.csv")
data.test <- read.csv("NA_Sample_Test1.csv")
data <- read.csv("NA_Sample2.csv")
idx <- createDataPartition(data$price, p = 0.8, list = F)
train <- data[idx,]
test <= data[-idx,]
test <- data[-idx,]
predict(model_xgb2, test)
model_xgb2
rm(list=ls())
load("model_xgb2")
load("model_xgb2.R")
model
rm(list=ls())
set.seed(1130)
library(caret)
load("model_xgb2.R") # model
setwd("~/Desktop/Justin/2018-1/DataMining/Project/")
data <- read.csv("NA_Sample2.csv")
colnames(data)
data <- read.csv("NA_Sample2.csv") ; data <- data[,-1]
idx <- createDataPartition(data$price, p = 0.8, list = F)
train <- data[idx,]
test <- data[-idx,]
predict(model_xgb2, test)
predict(model, test)
head(train)
colnames(train)
colnames(test)
idx <- createDataPartition(data$price, p = 0.8, list = F)
train <- data[idx,]
test <- data[-idx,-50]
colnames(test)
result <- data[, 50]
predict(model, test)
rm(list=ls())
set.seed(1130)
library(caret)
load("model_xgb2.R") # model
setwd("~/Desktop/Justin/2018-1/DataMining/Project/")
data <- read.csv("NA_Sample2.csv") ; data <- data[,-1]
idx <- createDataPartition(data$price, p = 0.8, list = F)
train <- data[idx,]
test <- data[-idx,-50]
colnames(Train)
colnames(train)
colnames(test)
idx <- createDataPartition(data$price, p = 0.8, list = F)
train <- data[idx,]
test <- data[-idx,-50]
result <- data[, 50]
predict(model, test)
model
data <- read.csv("NA_Sample2.csv")
idx <- createDataPartition(data$price, p = 0.8, list = F)
train <- data[idx,]
test <- data[-idx,-50]
result <- data[, 50]
predict(model, test)
colnames(train)
colnames(test)
setwd("~/Desktop/Justin/2018-1/DataMining/Project/")
data <- read.csv("NA_Sample2.csv")
idx <- createDataPartition(data$price, p = 0.8, list = F)
train <- data[idx,]
test <- data[-idx,-51]
result <- data[, 51]
predict(model, test)
predict.xgb <- predict(model, test)
RMSE(result, price.xgb)
predict.xgb <- predict(model, test)
RMSE(result, predict.xgb)
predict.xgb <- predict(model, test)
RMSE(result, predict.xgb)
R2(result, predict.xgb)
predict.xgb <- predict(model, test)
RMSE(result, predict.xgb)
R2(result, predict.xgb)
result
load("model_xgb1.R")
rm(list=ls())
load("model_xgb1.R")
load("model_xgb2.R") # model
rm(list=ls())
setwd("~/Desktop/Justin/2018-1/Multivariate_Statistical_Analysis/Data")
Sys.setlocale(category = "LC_CTYPE", locale = "ko_KR.UTF-8")
setwd("~/Desktop/Justin/2018-1/Multivariate_Statistical_Analysis/Data")
# 1
data <- read.csv("Bulls.csv")
pca <- prcomp(data[, c(-1, -2)], scale = TRUE)
pca
# 2
screeplot(pca, type = "l")
# 3
pca
# 4
biplot(pca, main = "Biplot")
# 5
library(ggplot2)
y <- data.frame(data$Bread, pca $x[, c(1, 2)])
y$data.Breed <- as.factor(y$data.Breed)
y <- data.frame(data$Bread, pca $x[, c(1, 2)])
y <- data.frame(data$Bread, pca$x[, c(1, 2)])
data
colnames(data)
pca$x
pca$x[, c(1,2)]
y <- data.frame(data$vreed, pca$x[, c(1,2)])
y <- data.frame(data$breed, pca$x[, c(1,2)])
y <- data.frame(data$Breed, pca$x[, c(1,2)])
# 5
library(ggplot2)
y <- data.frame(data$Breed, pca$x[, c(1,2)])
y$data.Breed <- as.factor(y$data.Breed)
ggplot(y, aes(x = PC1, y = PC2, group = data.Breed)) +
geom_point(aes(shape = data.Breed, colour = data.Breed), size = 6)
# 6
qqnorm(y$PC1) ; qqline(y$PC1)
rm(list=ls())
Sys.setlocale(category = "LC_CTYPE", locale = "ko_KR.UTF-8")
setwd("~/Desktop/Justin/2018-1/Multivariate_Statistical_Analysis/Data")
# 1
data <- read.csv("Bulls.csv")
pca <- prcomp(data[, c(-1, -2)], scale = TRUE)
pca
# 2
screeplot(pca, type = "l")
# 총분산을 설명하는 비중은, 주성분이 1개일 때, 약 80.82% 설명하고,
# 주성분이 2개일 때, 약 99.96% 설명한다. 주성분이 1개일 때도 높은 데이터 설명력을 가지지만,
# 약 99.96% 수준으로, 거의 모든 부분을 설명할 수 있는 주성분은 2개이므로 2개를 선택하는게 타당하다고 생각한다.
# 3
pca
# 상관관계행렬이
# 주성분 개수가 1개일 때, BkFAT을 제외한 나머지 요소는 모두 음수값이다.
# 따라서, BkFat을 제외한 나머지 요소가 증가한다면, 주성분이 감소될 수 있다고 판단할 수 있다.
# 주성분 개수가 2개일 때, PrctFFB, YrHGT 값은 음수, 나머지 요소는 양수이다.
# 이를 통해서, PrctFFB, YrHgt값을 제외한 나머지 요소가 증가한다면, 주성분이 증가할 수 있다고 판단할 수 있다.
# 4
biplot(pca, main = "Biplot")
# ㅇㄹㅇㄹ
# FtFrBody, SaleHt, Frame, YrHgt 는 상관관계가 높다고 판단할 수 있다.
# BkFat은 다른 종목들과 비교적 상관관계가 적다고 판단할 수 있다.
# 각 종목들의 화살표에 근접할수록, 해당 종목과 연관성이 높다고 판단할 수 있다.
# 중앙에서 멀어질수록 값이 높다고 판단할 수 있다.
# 5
library(ggplot2)
y <- data.frame(data$Breed, pca$x[, c(1,2)])
y$data.Breed <- as.factor(y$data.Breed)
ggplot(y, aes(x = PC1, y = PC2, group = data.Breed)) +
geom_point(aes(shape = data.Breed, colour = data.Breed), size = 6)
# 6
qqnorm(y$PC1) ; qqline(y$PC1)
data=read.csv("open_closed.csv")
pairs(data)
cor(data)
pca = prcomp(data, scale=T)
summary(pca)
plot(pca, type='l')
data_s=scale(data)
pca <- prcomp(data)
summary(pca)
# 1
data <- read.csv("Bulls.csv")
pca <- prcomp(data[, c(-1, -2)], scale = TRUE)
pca
summary(pca)
pca3 <- prcomp(data[, c(-1, -2)])
summary(pca3)
pca <- prcomp(data[, c(-1, -2)], scale = FALSE)
summary(pca)
pca2 <- prcomp(data[, c(-1, -2)], scale = TRUE)
summary(pca2)
# 총분산을 설명하는 비중은, 주성분이 1개일 때, 약 80.82% 설명하고,
# 주성분이 2개일 때, 약 99.96% 설명한다. 주성분이 1개일 때도 높은 데이터 설명력을 가지지만,
# 약 99.96% 수준으로, 거의 모든 부분을 설명할 수 있는 주성분은 2개이므로 2개를 선택하는게 타당하다고 생각한다.
screeplot(pca2, type = "l")
# 2
screeplot(pca, type = "l")
# 총분산을 설명하는 비중은, 주성분이 1개일 때, 약 80.82% 설명하고,
# 주성분이 2개일 때, 약 99.96% 설명한다. 주성분이 1개일 때도 높은 데이터 설명력을 가지지만,
# 약 99.96% 수준으로, 거의 모든 부분을 설명할 수 있는 주성분은 2개이므로 2개를 선택하는게 타당하다고 생각한다.
screeplot(pca2, type = "l")
# 2
screeplot(pca, type = "l")
data=read.csv("open_closed.csv")
data_s=scale(data)
plot(as.matrix(data_s) %*% pca$rotation[,1] ,pca$x[,1])
data=read.csv("open_closed.csv")
pairs(data)
cor(data)
pca = prcomp(data, scale=T)
summary(pca)
plot(pca, type='l')
data_s=scale(data)
plot(as.matrix(data_s) %*% pca$rotation[,1] ,pca$x[,1])
abline(0,1)
library(HSAUR2)
heptathlon$hurdles = with(heptathlon, max(hurdles)-hurdles)
heptathlon$run200m = with(heptathlon, max(run200m)-run200m)
heptathlon$run800m = with(heptathlon, max(run800m)-run800m)
score = which(colnames(heptathlon) == "score")
round(cor(heptathlon[,-score]), 2)
plot(heptathlon[,-score])
library(psych)
pairs.panels(heptathlon[,-score])
idx = heptathlon$hurdles==min(heptathlon$hurdles)
heptathlon[idx,]
pairs(heptathlon[,-score],col=idx+1, pch=idx+1)
heptathlon2 = heptathlon[-grep("PNG", rownames(heptathlon)),]
round(cor(heptathlon2[,-score]), 2)
pairs.panels(heptathlon2[,-score])
heptathlon_pca = prcomp(heptathlon2[, -score], scale = TRUE)
print(heptathlon_pca)
summary(heptathlon_pca)
heptathlon_pca$rotation
plot(heptathlon_pca,type="l")
cor(heptathlon2$score, heptathlon_pca$x[,1])
plot(heptathlon2$score, heptathlon_pca$x[,1])
tmp = heptathlon[,-score]
rownames(tmp) = abbreviate(gsub(" \\(.*", "", rownames(tmp)))
biplot(prcomp(tmp, scale=TRUE), col=c("black", "darkgray"),
xlim = c(-0.5, 0.7), cex=0.7)
data = read.csv("face100.csv" , stringsAsFactors=F)
dim(data)
# extract the images data
im.train = data$Image
data$Image = NULL
# each image is a vector of 96*96 pixels (96*96 = 9216).
im.train2 = c()
for (i in 1:dim(data)[1]){
im.train2 = rbind(im.train2, as.integer(unlist(strsplit(im.train[i], " "))))
}
# show picture
idx=23
im = matrix(data=rev(im.train2[idx,]), nrow=96, ncol=96)
image(1:96, 1:96, im, col=gray((0:255)/255))
# Apply PCA
pca = prcomp(im.train2, scale = TRUE) ## using correlation matrix
print(pca)
summary(pca)
plot(pca, type = "l")
pca$x
dim(pca$x)
# Image reconstruction
dim = 10
restr = pca$x[,1:dim] %*% t(pca$rotation[,1:dim])
# unscale and uncenter the data
if(all(pca$scale != FALSE)){
restr = scale(restr, center = FALSE , scale=1/pca$scale)
}
if(all(pca$center != FALSE)){
restr = scale(restr, center = -1 * pca$center, scale=FALSE)
}
# plot your original image and reconstructed image
idx=23
par(mfcol=c(1,2), mar=c(1,1,2,1))
im = matrix(data=rev(im.train2[idx,]), nrow=96, ncol=96)
image(1:96, 1:96, im, col=gray((0:255)/255))
rst = matrix(data=rev(restr[idx,]), nrow=96, ncol=96)
image(1:96, 1:96, rst, col=gray((0:255)/255))
rm(list=ls())
# 2
par(mfcol = c(1,2))
screeplot(pca)
Sys.setlocale(category = "LC_CTYPE", locale = "ko_KR.UTF-8")
setwd("~/Desktop/Justin/2018-1/Multivariate_Statistical_Analysis/Data")
# 1
data <- read.csv("Bulls.csv")
pca <- prcomp(data[, c(-1, -2)], scale = FALSE)
summary(pca)
pca2 <- prcomp(data[, c(-1, -2)], scale = TRUE)
summary(pca2)
pca
pca2
# 2
par(mfcol = c(1,2))
screeplot(pca)
screeplot(pca, type = "l")
# 총분산을 설명하는 비중은, 주성분이 1개일 때, 약 80.82% 설명하고,
# 주성분이 2개일 때, 약 99.96% 설명한다. 주성분이 1개일 때도 높은 데이터 설명력을 가지지만,
# 약 99.96% 수준으로, 거의 모든 부분을 설명할 수 있는 주성분은 2개이므로 2개를 선택하는게 타당하다고 생각한다.
screeplot(pca2)
screeplot(pca2, type = "l")
